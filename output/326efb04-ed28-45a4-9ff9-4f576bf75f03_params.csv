Parameter,Value
uid,326efb04-ed28-45a4-9ff9-4f576bf75f03
date_completed,2024-07-17 13:45:04.059796
algorithm,PPO with continous action gaussian policy
total_episodes,200
gamma,0.99
lambda,0.99
num_actions,2
num_states,3
HIDDEN_LAYER_SIZES,"(256, 256)"
target_kl,0.01
train_policy_iterations,80
train_value_iterations,80
max_env_steps,100
latency,0
policy_learning_rate,0.0003
value_function_learning_rate,0.001
clip_ration,0.1
env.delta_t,0.1
env.max_t,10.0
env_type,SimpleControlledFixedEnv
env.fixed_errors_flags,"[True, True, True, True, False, False, True, True, True, True, True, True]"
env.fixed_error_ctrl_pump,"[0, 0, 0, 0]"
env.fixed_error_ctrl_alice,"[3.141592653589793, 0, 0, 0]"
env.fixed_error_ctrl_bob,"[0, 0, 0, 0]"
total_training_time,61.38297772407532
avg_time_per_ep,61.38297772407532
baseline_reward,-50.93845176557012
convergence_reward,-32.96258684720233
note,
