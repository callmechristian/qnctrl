{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 14:37:53.188083: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-26 14:37:53.265342: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from environment.models.simple_control import SimpleControlledEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import namedtuple, deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    ''' Base agent class, used as a parent class\n",
    "\n",
    "        Args:\n",
    "            n_actions (int): number of actions\n",
    "\n",
    "        Attributes:\n",
    "            n_actions (int): where we store the number of actions\n",
    "            last_action (np.array): last action taken by the agent\n",
    "    '''\n",
    "    def __init__(self, n_actions: int):\n",
    "        self.n_actions = n_actions\n",
    "        self.last_action = None\n",
    "\n",
    "    def forward(self, state: np.ndarray):\n",
    "        ''' Performs a forward computation '''\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        ''' Performs a backward pass on the network '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
    "    def __init__(self, n_actions: int):\n",
    "        super(RandomAgent, self).__init__(n_actions)\n",
    "\n",
    "    def forward(self, state: np.ndarray) -> int:\n",
    "        ''' Compute an action uniformly at random across n_actions possible\n",
    "            choices\n",
    "\n",
    "            Returns:\n",
    "                action np.array(int): the random action for each angle\n",
    "        '''\n",
    "        action = []\n",
    "        for i in range(self.n_actions):\n",
    "            action.append(np.random.randint(-2*np.pi, 2*np.pi))\n",
    "        self.last_action = np.array(action)\n",
    "        return self.last_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class for the experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, maximum_length):\n",
    "        # Create buffer of maximum length\n",
    "        self.buffer = deque(maxlen=maximum_length)\n",
    "        self.latest_experience = None\n",
    "\n",
    "    def append(self, experience):\n",
    "        # Append experience to the buffer\n",
    "        if(self.latest_experience is not None):\n",
    "            self.buffer.append(self.latest_experience)\n",
    "\n",
    "        self.latest_experience = experience\n",
    "\n",
    "    def __len__(self):\n",
    "        # overload len operator\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\" Function used to sample experiences from the buffer.\n",
    "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
    "            rewards, next states and done variables.\n",
    "        \"\"\"\n",
    "        # If we try to sample more elements that what are available from the\n",
    "        # buffer we raise an error\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
    "        \n",
    "        # combined experience replay\n",
    "        # # inclued latest experience in the sampled batch\n",
    "                    \n",
    "        batch = random.sample(self.buffer, n - 1)\n",
    "        batch.append(self.latest_experience)\n",
    "\n",
    "        # batch is a list of size n, where each element is an Experience tuple\n",
    "        # of 5 elements. To convert a list of tuples into\n",
    "        # a tuple of list we do zip(*batch). In this case this will return a\n",
    "        # tuple of 5 elements where each element is a list of n elements.\n",
    "\n",
    "        return zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(keras.models.Model):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.input_layer = keras.layers.keras.layers.Dense(64, activation='relu')\n",
    "        self.hidden_layer1 = keras.layers.keras.layers.Dense(16, activation='relu')\n",
    "        \n",
    "        self.hidden_value_layer1 = keras.layers.keras.layers.Dense(128, activation='relu')\n",
    "        self.hidden_advantage_layer1 = keras.layers.keras.layers.Dense(128, activation='relu')\n",
    "        self.value_layer = keras.layers.keras.layers.Dense(1)\n",
    "        self.advantage_layer = keras.layers.keras.layers.Dense(output_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        _in = keras.layers.ReLU()(self.input_layer(x))\n",
    "        l1 = keras.layers.ReLU()(self.hidden_layer1(_in))\n",
    "\n",
    "        v1 = keras.layers.ReLU()(self.hidden_value_layer1(l1))\n",
    "        v2 = self.value_layer(v1)\n",
    "\n",
    "        a1 = keras.layers.ReLU()(self.hidden_advantage_layer1(l1))\n",
    "        a2 = self.advantage_layer(a1)\n",
    "\n",
    "        q = v2 + a2 - tf.reduce_mean(a2, axis=-1, keepdims=True)\n",
    "        return q\n",
    "    \n",
    "    def compute_q_values(self, states, actions):\n",
    "        q_values = self(states)\n",
    "        selected_q_values = tf.gather(q_values, actions, axis=1)\n",
    "        return selected_q_values\n",
    "\n",
    "    def update(self, optimizer, loss_function, predicted_q_values, target_values):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = loss_function(predicted_q_values, target_values)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "def epsilon_decay(epsilon_min, epsilon_max, decay_step, k):\n",
    "    decayed_epsilon = max(epsilon_min, epsilon_max * (epsilon_min / epsilon_max) ** ((k - 1)/(decay_step - 1)))\n",
    "    return decayed_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent(Agent):\n",
    "    def __init__(self, state_size, action_size, replay_length=5000, batch_size=64, gamma=0.99, learning_rate=1e-3, n_episodes=800):\n",
    "        super(DDPGAgent, self).__init__(action_size)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_episodes = n_episodes\n",
    "        self.episode = 0\n",
    "        self.epsilon = 1\n",
    "        self.Z = 0.9*self.n_episodes\n",
    "        self.epsilon_max = 0.99\n",
    "        self.epsilon_min = 0.05\n",
    "        \n",
    "        # env specific\n",
    "        self.min_action = -2*np.pi\n",
    "        self.max_action = 2*np.pi\n",
    "        \n",
    "        # step 1:\n",
    "        ### Create critic network\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.critic_q_network = self._build_network(state_size, action_size)\n",
    "        self.critic_target_network = self._build_network(state_size, action_size)\n",
    "        self.critic_target_network.set_weights(self.q_network.get_weights())\n",
    "        ### Create actor network\n",
    "        self.actor_q_network = self._build_network(state_size, action_size)\n",
    "        self.actor_target_network = self._build_network(state_size, action_size)\n",
    "        self.actor_target_network.set_weights(self.q_network.get_weights())\n",
    "        \n",
    "        # step 2:\n",
    "        ### Create Experience replay buffer\n",
    "        self.buffer = ExperienceReplayBuffer(maximum_length=replay_length)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        ### Agent init\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        ### Steps\n",
    "        self.target_update_rate = int(replay_length/batch_size) # suggested as tip\n",
    "        self.steps = 0  # Counter for steps taken\n",
    "        \n",
    "    def _build_network(self, state_size, action_size):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(24, input_dim=state_size, activation='relu'))\n",
    "        model.add(keras.layers.Dense(24, activation='relu'))\n",
    "        model.add(keras.layers.Dense(action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=self.optimizer)\n",
    "        return model\n",
    "\n",
    "    def forward(self, state):\n",
    "        # step 7:\n",
    "        # take noisy continuous action a_t at s_t     \n",
    "        q = self.actor_q_network.predict(state)\n",
    "        # loop over the actions and add noise\n",
    "        for i in range(len(q)):\n",
    "            # assign noise\n",
    "            noise = np.random.normal(self.min_action, 0.1, self.max_action)\n",
    "            q[i] = q[i] + noise\n",
    "\n",
    "        return np.argmax(q, axis=1).reshape(1, 1)\n",
    "     \n",
    "    def sample(self, state):\n",
    "        if not isinstance(state[0], np.float32):\n",
    "            state = np.array(state[0])\n",
    "        q = self.q_network.predict(state)\n",
    "        return np.argmax(q)\n",
    "\n",
    "    def backward(self):\n",
    "        if len(self.buffer.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # step 9:\n",
    "        # Sample a batch of experiences from the buffer\n",
    "        batch = Experience(*self.buffer.sample_batch(self.batch_size))        \n",
    "        batch_mask = ~np.array(batch.done, dtype=bool)\n",
    "\n",
    "        next_state = np.stack(batch.next_state)\n",
    "        states = np.stack(batch.state)\n",
    "        actions = np.concatenate(batch.action)\n",
    "        rewards = np.concatenate(batch.reward)        \n",
    "\n",
    "        # step 10:\n",
    "        # Compute target values for each experience in the batch\n",
    "        target_values = tf.where(batch_mask, rewards, 0)\n",
    "        target_values = tf.where(~batch_mask, target_values + self.gamma * self.critic_target_network.predict(next_state), target_values)\n",
    "\n",
    "\n",
    "        # step 11:\n",
    "        # Compute predicted Q-values for the states and actions in the batch\n",
    "        predicted_q_values = self.critic_q_network.predict(states)\n",
    "        predicted_q_values = predicted_q_values[tf.range(len(actions)), actions]\n",
    "\n",
    "        # Update critic Q-network weights using the computed values (backward pass SGD on the MSE loss)\n",
    "        # self.critic_q_network.fit(np.stack(states, actions), target_values, epochs=1, verbose=0)\n",
    "        def loss_function(predicted_q_values, target_values):\n",
    "            _ret = target_values - \n",
    "            return keras.losses.MeanSquaredError()(predicted_q_values, target_values)\n",
    "        self.critic_q_network.update(self.optimizer, loss_function, predicted_q_values, target_values)\n",
    "\n",
    "        # step 12:\n",
    "        if self.steps % self.target_update_rate == self.target_update_rate - 1:\n",
    "            # step 13:\n",
    "            # update critic\n",
    "            pass\n",
    "        \n",
    "        # step 16:\n",
    "        # Increment steps counter\n",
    "        self.steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n",
    "\n",
    "\n",
    "def faster_running_average(x, N, last_average):\n",
    "    if len(x) > N:\n",
    "        return last_average + (1./N)*(x[-1] - x[-N - 1])\n",
    "    else:\n",
    "        return sum(x)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 14:37:54.261411: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DDPGAgent' object has no attribute 'q_network'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m episode_number_of_steps \u001b[38;5;241m=\u001b[39m []   \u001b[38;5;66;03m# this list contains the number of steps per episode\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# agent initialization\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# agent = RandomAgent(n_actions) # random\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# step 1 & 2:\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDDPGAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mDDPGAgent.__init__\u001b[0;34m(self, state_size, action_size, replay_length, batch_size, gamma, learning_rate, n_episodes)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_q_network \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_network(state_size, action_size)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target_network \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_network(state_size, action_size)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target_network\u001b[38;5;241m.\u001b[39mset_weights(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_network\u001b[49m\u001b[38;5;241m.\u001b[39mget_weights())\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m### Create actor network\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_q_network \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_network(state_size, action_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DDPGAgent' object has no attribute 'q_network'"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_episodes = 1000                 # Number of episodes # 100 - 1000\n",
    "discount_factor = 0.99            # Value of the discount factor\n",
    "n_ep_running_average = 50         # Running average of 50 episodes\n",
    "n_actions = 4                     # Number of available actions (nr of angles)\n",
    "dim_state = 2                     # State dimensionality\n",
    "replay_size = 5000                # 5000 - 30000\n",
    "batch_size = 64                   # 4 - 128\n",
    "learning_rate = 1e-4              # 1e-3 - 1e-4\n",
    "target_reward = 0                 # specified in lab\n",
    "max_env_steps = 1000              # to stop the episode\n",
    "\n",
    "# We will use these variables to compute the average episodic reward and\n",
    "# the average number of steps per episode\n",
    "episode_reward_list = []       # this list contains the total reward per episode\n",
    "episode_number_of_steps = []   # this list contains the number of steps per episode\n",
    "\n",
    "# agent initialization\n",
    "# agent = RandomAgent(n_actions) # random\n",
    "# step 1 & 2:\n",
    "agent = DDPGAgent(dim_state, n_actions, replay_size, batch_size=batch_size, gamma=discount_factor, learning_rate=learning_rate, n_episodes=N_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleControlledEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "### Training process\n",
    "from tqdm import trange\n",
    "# trange is an alternative to range in python, from the tqdm library\n",
    "# It shows a nice progression bar that you can update with useful information\n",
    "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "actual_episodes = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate getting higher-quality training data, you may reduce the scale of the noise over the course of training. (We do not do this in our implementation, and keep noise scale fixed throughout.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At test time, to see how well the policy exploits what it has learned, we do not add noise to the actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DDPG implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning (set with the start_steps keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal DDPG exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = 0.\n",
    "avg_steps = 0.\n",
    "# step 3:\n",
    "# episode loop\n",
    "for i in EPISODES:\n",
    "    # Reset environment data and initialize variables\n",
    "    done = False\n",
    "    # step 4:\n",
    "    state, _ = env.reset()\n",
    "    state = np.array(state, dtype=np.float32)\n",
    " \n",
    "    total_episode_reward = 0.\n",
    "    # step 5:\n",
    "    t = 0\n",
    "    # step 6: \n",
    "    # environment loop\n",
    "    while not done:\n",
    "        # necessary for lunar lander. It doesn't implement a default\n",
    "        # max-timesteps and rover hovers forever\n",
    "\n",
    "        # step 7:\n",
    "        # noisy actor action\n",
    "        action = agent.forward(state)\n",
    "\n",
    "        # step 8:\n",
    "        # Execute action in the environment and append\n",
    "        next_state, reward, done = env.step(action)\n",
    "        # append to buffer\n",
    "        agent.buffer.append(Experience(state, action, np.array([reward]), next_state, done))\n",
    "        \n",
    "        # step 9-15: see function definition\n",
    "        agent.backward()\n",
    "        \n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # step 16: Update state for next iteration\n",
    "        state = next_state\n",
    "        t += 1\n",
    "\n",
    "    # Append episode reward and total number of steps\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "    episode_number_of_steps.append(t)\n",
    "\n",
    "    avg_reward = faster_running_average(episode_reward_list, n_ep_running_average, avg_reward)\n",
    "    avg_steps = faster_running_average(episode_number_of_steps, n_ep_running_average, avg_steps)\n",
    "\n",
    "    agent.episode += 1\n",
    "\n",
    "    # Updates the tqdm update bar with fresh information\n",
    "    # (episode number, total reward of the last episode, total number of Steps\n",
    "    # of the last episode, average reward, average number of steps)\n",
    "    EPISODES.set_description(\n",
    "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{:.1f}\".format(\n",
    "        i, total_episode_reward, t,\n",
    "        avg_reward,\n",
    "        avg_steps)\n",
    "        )\n",
    "\n",
    "    actual_episodes += 1\n",
    "    \n",
    "    # stop if we hit reward threshold\n",
    "    if avg_reward >= target_reward:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
