{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 10:21:43.559653: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 10:21:43.741342: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from environment.models.simple_control import SimpleControlledEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import namedtuple, deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    ''' Base agent class, used as a parent class\n",
    "\n",
    "        Args:\n",
    "            n_actions (int): number of actions\n",
    "\n",
    "        Attributes:\n",
    "            n_actions (int): where we store the number of actions\n",
    "            last_action (np.array): last action taken by the agent\n",
    "    '''\n",
    "    def __init__(self, n_actions: int):\n",
    "        self.n_actions = n_actions\n",
    "        self.last_action = None\n",
    "\n",
    "    def forward(self, state: np.ndarray):\n",
    "        ''' Performs a forward computation '''\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        ''' Performs a backward pass on the network '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
    "    def __init__(self, n_actions: int):\n",
    "        super(RandomAgent, self).__init__(n_actions)\n",
    "\n",
    "    def forward(self, state: np.ndarray) -> int:\n",
    "        ''' Compute an action uniformly at random across n_actions possible\n",
    "            choices\n",
    "\n",
    "            Returns:\n",
    "                action np.array(int): the random action for each angle\n",
    "        '''\n",
    "        action = []\n",
    "        for i in range(self.n_actions):\n",
    "            action.append(np.random.randint(-2*np.pi, 2*np.pi))\n",
    "        self.last_action = np.array(action)\n",
    "        return self.last_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class for the experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, maximum_length):\n",
    "        # Create buffer of maximum length\n",
    "        self.buffer = deque(maxlen=maximum_length)\n",
    "        self.latest_experience = None\n",
    "\n",
    "    def append(self, experience):\n",
    "        # Append experience to the buffer\n",
    "        if(self.latest_experience is not None):\n",
    "            self.buffer.append(self.latest_experience)\n",
    "\n",
    "        self.latest_experience = experience\n",
    "\n",
    "    def __len__(self):\n",
    "        # overload len operator\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\" Function used to sample experiences from the buffer.\n",
    "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
    "            rewards, next states and done variables.\n",
    "        \"\"\"\n",
    "        # If we try to sample more elements that what are available from the\n",
    "        # buffer we raise an error\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
    "        \n",
    "        # combined experience replay\n",
    "        # # inclued latest experience in the sampled batch\n",
    "                    \n",
    "        batch = random.sample(self.buffer, n - 1)\n",
    "        batch.append(self.latest_experience)\n",
    "\n",
    "        # batch is a list of size n, where each element is an Experience tuple\n",
    "        # of 5 elements. To convert a list of tuples into\n",
    "        # a tuple of list we do zip(*batch). In this case this will return a\n",
    "        # tuple of 5 elements where each element is a list of n elements.\n",
    "\n",
    "        return zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(keras.models.Model):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.input_layer = keras.layers.keras.layers.Dense(64, activation='relu')\n",
    "        self.hidden_layer1 = keras.layers.keras.layers.Dense(16, activation='relu')\n",
    "        \n",
    "        self.hidden_value_layer1 = keras.layers.keras.layers.Dense(128, activation='relu')\n",
    "        self.hidden_advantage_layer1 = keras.layers.keras.layers.Dense(128, activation='relu')\n",
    "        self.value_layer = keras.layers.keras.layers.Dense(1)\n",
    "        self.advantage_layer = keras.layers.keras.layers.Dense(output_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        _in = keras.layers.ReLU()(self.input_layer(x))\n",
    "        l1 = keras.layers.ReLU()(self.hidden_layer1(_in))\n",
    "\n",
    "        v1 = keras.layers.ReLU()(self.hidden_value_layer1(l1))\n",
    "        v2 = self.value_layer(v1)\n",
    "\n",
    "        a1 = keras.layers.ReLU()(self.hidden_advantage_layer1(l1))\n",
    "        a2 = self.advantage_layer(a1)\n",
    "\n",
    "        q = v2 + a2 - tf.reduce_mean(a2, axis=-1, keepdims=True)\n",
    "        return q\n",
    "    \n",
    "    def compute_q_values(self, states, actions):\n",
    "        q_values = self(states)\n",
    "        selected_q_values = tf.gather(q_values, actions, axis=1)\n",
    "        return selected_q_values\n",
    "\n",
    "    def update(self, optimizer, loss_function, predicted_q_values, target_values):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = loss_function(predicted_q_values, target_values)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "def epsilon_decay(epsilon_min, epsilon_max, decay_step, k):\n",
    "    decayed_epsilon = max(epsilon_min, epsilon_max * (epsilon_min / epsilon_max) ** ((k - 1)/(decay_step - 1)))\n",
    "    return decayed_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "    def __init__(self, state_size, action_size, replay_length=5000, batch_size=64, gamma=0.99, learning_rate=1e-3, n_episodes=800):\n",
    "        super(DQNAgent, self).__init__(action_size)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_episodes = n_episodes\n",
    "        self.episode = 0\n",
    "        self.epsilon = 1\n",
    "        self.Z = 0.9*self.n_episodes\n",
    "        self.epsilon_max = 0.99\n",
    "        self.epsilon_min = 0.05\n",
    "        \n",
    "        # step 1:\n",
    "        ### Create network\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.q_network = self._build_network(state_size, action_size)\n",
    "        self.target_network = self._build_network(state_size, action_size)\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "        \n",
    "        # step 2:\n",
    "        ### Create Experience replay buffer\n",
    "        self.buffer = ExperienceReplayBuffer(maximum_length=replay_length)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        ### Agent init\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        ### Steps\n",
    "        self.target_update_rate = int(replay_length/batch_size) # suggested as tip\n",
    "        self.steps = 0  # Counter for steps taken\n",
    "        \n",
    "    def _build_network(self, state_size, action_size):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(24, input_dim=state_size, activation='relu'))\n",
    "        model.add(keras.layers.Dense(24, activation='relu'))\n",
    "        model.add(keras.layers.Dense(action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=self.optimizer)\n",
    "        return model\n",
    "\n",
    "    def forward(self, state):\n",
    "        # step 7:\n",
    "        # take epsilon-greedy action a_t at s_t     \n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon_max * (self.epsilon_min/self.epsilon_max) ** ((self.episode - 1)/(self.Z - 1)))\n",
    "            return np.random.randint(self.action_size, size=(1, 1))\n",
    "\n",
    "        q = self.q_network.predict(state)\n",
    "        \n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_max * (self.epsilon_min/self.epsilon_max) ** ((self.episode - 1)/(self.Z- 1)))\n",
    "\n",
    "        return np.argmax(q, axis=1).reshape(1, 1)\n",
    "     \n",
    "    def sample(self, state):\n",
    "        if not isinstance(state[0], np.float32):\n",
    "            state = np.array(state[0])\n",
    "        q = self.q_network.predict(state)\n",
    "        return np.argmax(q)\n",
    "\n",
    "    def backward(self):\n",
    "        if len(self.buffer.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # step 9:\n",
    "        # Sample a batch of experiences from the buffer\n",
    "        batch = Experience(*self.buffer.sample_batch(self.batch_size))        \n",
    "        batch_mask = ~np.array(batch.done, dtype=bool)\n",
    "\n",
    "        next_state = np.stack(batch.next_state)\n",
    "        states = np.stack(batch.state)\n",
    "        actions = np.concatenate(batch.action)\n",
    "        rewards = np.concatenate(batch.reward)        \n",
    "\n",
    "        # step 10:\n",
    "        # Compute target values for each experience in the batch\n",
    "        target_values = np.zeros((next_state.shape[0], 1), dtype=np.float32)\n",
    "\n",
    "        a_star = np.argmax(self.q_network.predict(next_state), axis=1)\n",
    "        target_values[batch_mask] = self.target_network.predict(next_state)[np.arange(len(a_star)), a_star][batch_mask]\n",
    "        target_values = rewards + self.gamma * target_values.squeeze(1)\n",
    "\n",
    "        # step 11:\n",
    "        # Compute predicted Q-values for the states and actions in the batch\n",
    "        predicted_q_values = self.q_network.predict(states)\n",
    "        predicted_q_values = predicted_q_values[np.arange(len(actions)), actions]\n",
    "\n",
    "        # Update Q-network weights using the computed values\n",
    "        self.q_network.fit(states, target_values, epochs=1, verbose=0)\n",
    "\n",
    "        # step 12:\n",
    "        # Implement the target network update if C steps have passed (Step 12)\n",
    "        if self.steps % self.target_update_rate == self.target_update_rate - 1:\n",
    "            self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        # step 13:\n",
    "        # Increment steps counter\n",
    "        self.steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n",
    "\n",
    "\n",
    "def faster_running_average(x, N, last_average):\n",
    "    if len(x) > N:\n",
    "        return last_average + (1./N)*(x[-1] - x[-N - 1])\n",
    "    else:\n",
    "        return sum(x)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.optimizers.optimizer_experimental.adam.Adam object at 0x7f5d6c1d2310>\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_episodes = 1000                            # Number of episodes # 100 - 1000\n",
    "discount_factor = 0.99                       # Value of the discount factor\n",
    "n_ep_running_average = 50                    # Running average of 50 episodes\n",
    "n_actions = 12                                # Number of available actions\n",
    "dim_state = 3                                 # State dimensionality\n",
    "replay_size = 5000                          # 5000 - 30000\n",
    "batch_size = 64                              # 4 - 128\n",
    "learning_rate = 1e-4                         # 1e-3 - 1e-4\n",
    "target_reward = 0                               # specified in lab\n",
    "max_env_steps = 1000                          # to stop the episode\n",
    "\n",
    "# We will use these variables to compute the average episodic reward and\n",
    "# the average number of steps per episode\n",
    "episode_reward_list = []       # this list contains the total reward per episode\n",
    "episode_number_of_steps = []   # this list contains the number of steps per episode\n",
    "\n",
    "# agent initialization\n",
    "# agent = RandomAgent(n_actions) # random\n",
    "# step 1 & 2:\n",
    "agent = DQNAgent(dim_state, n_actions, replay_size, batch_size=batch_size, gamma=discount_factor, learning_rate=learning_rate, n_episodes=N_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleControlledEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_episodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# trange is an alternative to range in python, from the tqdm library\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# It shows a nice progression bar that you can update with useful information\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m EPISODES \u001b[38;5;241m=\u001b[39m trange(\u001b[43mN_episodes\u001b[49m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m actual_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N_episodes' is not defined"
     ]
    }
   ],
   "source": [
    "### Training process\n",
    "from tqdm import trange\n",
    "# trange is an alternative to range in python, from the tqdm library\n",
    "# It shows a nice progression bar that you can update with useful information\n",
    "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "actual_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = 0.\n",
    "avg_steps = 0.\n",
    "# step 3:\n",
    "# episode loop\n",
    "for i in EPISODES:\n",
    "    # Reset environment data and initialize variables\n",
    "    done = False\n",
    "    # step 4:\n",
    "    state, _ = env.reset()\n",
    "    state = np.array(state, dtype=np.float32)\n",
    " \n",
    "    total_episode_reward = 0.\n",
    "    # step 5:\n",
    "    t = 0\n",
    "    # step 6: \n",
    "    # environment loop\n",
    "    while not done:\n",
    "        # necessary for lunar lander. It doesn't implement a default\n",
    "        # max-timesteps and rover hovers forever\n",
    "\n",
    "        # step 7:\n",
    "        # epsilon-greedy action\n",
    "        action = agent.forward(state)\n",
    "\n",
    "        # Get next state and reward. The done variable\n",
    "        # will be True if you reached the goal position,\n",
    "        # False otherwise\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_state = np.array(next_state, dtype=np.float32)\n",
    "        # append to buffer\n",
    "        agent.buffer.append(Experience(state, action, np.array([reward], dtype=np.float32), next_state, done))\n",
    "            \n",
    "        agent.backward()\n",
    "        \n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "        t += 1\n",
    "\n",
    "    # Append episode reward and total number of steps\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "    episode_number_of_steps.append(t)\n",
    "\n",
    "    avg_reward = faster_running_average(episode_reward_list, n_ep_running_average, avg_reward)\n",
    "    avg_steps = faster_running_average(episode_number_of_steps, n_ep_running_average, avg_steps)\n",
    "\n",
    "    agent.episode += 1\n",
    "\n",
    "    # Updates the tqdm update bar with fresh information\n",
    "    # (episode number, total reward of the last episode, total number of Steps\n",
    "    # of the last episode, average reward, average number of steps)\n",
    "    EPISODES.set_description(\n",
    "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{:.1f}\".format(\n",
    "        i, total_episode_reward, t,\n",
    "        avg_reward,\n",
    "        avg_steps)\n",
    "        )\n",
    "\n",
    "    actual_episodes += 1\n",
    "    \n",
    "    # stop if we hit reward threshold\n",
    "    if avg_reward >= target_reward:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
