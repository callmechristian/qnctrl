{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 08:42:33.273175: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-08 08:42:33.334057: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from environment.models.simple_control_fixed import SimpleControlledFixedEnv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import namedtuple, deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    ''' Base agent class, used as a parent class\n",
    "\n",
    "        Args:\n",
    "            n_actions (int): number of actions\n",
    "\n",
    "        Attributes:\n",
    "            n_actions (int): where we store the number of actions\n",
    "            last_action (np.array): last action taken by the agent\n",
    "    '''\n",
    "    def __init__(self, n_actions: int):\n",
    "        self.n_actions = n_actions\n",
    "        self.last_action = None\n",
    "\n",
    "    def forward(self, state: np.ndarray):\n",
    "        ''' Performs a forward computation '''\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        ''' Performs a backward pass on the network '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
    "    def __init__(self, n_actions: int):\n",
    "        super(RandomAgent, self).__init__(n_actions)\n",
    "\n",
    "    def forward(self, state: np.ndarray) -> int:\n",
    "        ''' Compute an action uniformly at random across n_actions possible\n",
    "            choices\n",
    "\n",
    "            Returns:\n",
    "                action np.array(int): the random action for each angle\n",
    "        '''\n",
    "        action = []\n",
    "        for i in range(self.n_actions):\n",
    "            action.append(np.random.randint(-2*np.pi, 2*np.pi))\n",
    "        self.last_action = np.array(action)\n",
    "        return self.last_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class for the experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, maximum_length):\n",
    "        # Create buffer of maximum length\n",
    "        self.buffer = deque(maxlen=maximum_length)\n",
    "        self.latest_experience = None\n",
    "\n",
    "    def append(self, experience):\n",
    "        # Append experience to the buffer\n",
    "        if(self.latest_experience is not None):\n",
    "            self.buffer.append(self.latest_experience)\n",
    "\n",
    "        self.latest_experience = experience\n",
    "\n",
    "    def __len__(self):\n",
    "        # overload len operator\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\" Function used to sample experiences from the buffer.\n",
    "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
    "            rewards, next states and done variables.\n",
    "        \"\"\"\n",
    "        # If we try to sample more elements that what are available from the\n",
    "        # buffer we raise an error\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
    "        \n",
    "        # combined experience replay\n",
    "        # # inclued latest experience in the sampled batch\n",
    "                    \n",
    "        batch = random.sample(self.buffer, n - 1)\n",
    "        batch.append(self.latest_experience)\n",
    "\n",
    "        # batch is a list of size n, where each element is an Experience tuple\n",
    "        # of 5 elements. To convert a list of tuples into\n",
    "        # a tuple of list we do zip(*batch). In this case this will return a\n",
    "        # tuple of 5 elements where each element is a list of n elements.\n",
    "\n",
    "        return zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(keras.models.Model):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.input_layer = keras.layers.keras.layers.Dense(64, activation='relu')\n",
    "        self.hidden_layer1 = keras.layers.keras.layers.Dense(16, activation='relu')\n",
    "        \n",
    "        self.hidden_value_layer1 = keras.layers.keras.layers.Dense(128, activation='relu')\n",
    "        self.hidden_advantage_layer1 = keras.layers.keras.layers.Dense(128, activation='relu')\n",
    "        self.value_layer = keras.layers.keras.layers.Dense(1)\n",
    "        self.advantage_layer = keras.layers.keras.layers.Dense(output_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        _in = keras.layers.ReLU()(self.input_layer(x))\n",
    "        l1 = keras.layers.ReLU()(self.hidden_layer1(_in))\n",
    "\n",
    "        v1 = keras.layers.ReLU()(self.hidden_value_layer1(l1))\n",
    "        v2 = self.value_layer(v1)\n",
    "\n",
    "        a1 = keras.layers.ReLU()(self.hidden_advantage_layer1(l1))\n",
    "        a2 = self.advantage_layer(a1)\n",
    "\n",
    "        q = v2 + a2 - tf.reduce_mean(a2, axis=-1, keepdims=True)\n",
    "        return q\n",
    "    \n",
    "    def compute_q_values(self, states, actions):\n",
    "        q_values = self(states)\n",
    "        selected_q_values = tf.gather(q_values, actions, axis=1)\n",
    "        return selected_q_values\n",
    "\n",
    "    def update(self, optimizer, loss_function, predicted_q_values, target_values):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = loss_function(predicted_q_values, target_values)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "def epsilon_decay(epsilon_min, epsilon_max, decay_step, k):\n",
    "    decayed_epsilon = max(epsilon_min, epsilon_max * (epsilon_min / epsilon_max) ** ((k - 1)/(decay_step - 1)))\n",
    "    return decayed_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent(Agent):\n",
    "    def __init__(self, state_size, action_size, replay_length=5000, batch_size=64, gamma=0.99, learning_rate=1e-3, n_episodes=800):\n",
    "        super(DDPGAgent, self).__init__(action_size)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_episodes = n_episodes\n",
    "        self.episode = 0\n",
    "        self.epsilon = 1\n",
    "        self.Z = 0.9*self.n_episodes\n",
    "        self.epsilon_max = 0.99\n",
    "        self.epsilon_min = 0.05\n",
    "        \n",
    "        # env specific\n",
    "        self.min_action = -2*np.pi\n",
    "        self.max_action = 2*np.pi\n",
    "        \n",
    "        # step 1:\n",
    "        ### Create critic network\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.critic_q_network = self._build_network(state_size, action_size)\n",
    "        self.critic_target_network = self._build_network(state_size, action_size)\n",
    "        self.critic_target_network.set_weights(self.critic_q_network.get_weights())\n",
    "        ### Create actor network\n",
    "        self.actor_q_network = self._build_network(state_size, action_size)\n",
    "        self.actor_target_network = self._build_network(state_size, action_size)\n",
    "        self.actor_target_network.set_weights(self.actor_q_network.get_weights())\n",
    "        \n",
    "        # step 2:\n",
    "        ### Create Experience replay buffer\n",
    "        self.buffer = ExperienceReplayBuffer(maximum_length=replay_length)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        ### Agent init\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        ### Steps\n",
    "        self.target_update_rate = int(replay_length/batch_size) # suggested as tip\n",
    "        self.steps = 0  # Counter for steps taken\n",
    "        \n",
    "    def _build_network(self, state_size, action_size):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(24, input_dim=state_size, activation='relu'))\n",
    "        model.add(keras.layers.Dense(24, activation='relu'))\n",
    "        model.add(keras.layers.Dense(action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=self.optimizer)\n",
    "        return model\n",
    "\n",
    "    def forward(self, state):\n",
    "        # step 7:\n",
    "        # take noisy continuous action a_t at s_t   \n",
    "        q = self.actor_q_network.predict(state)\n",
    "        # loop over the actions and add noise\n",
    "        for i in range(len(q)):\n",
    "            # assign noise\n",
    "            noise = np.random.uniform(self.min_action, self.max_action)\n",
    "            q[i] = q[i] + noise\n",
    "\n",
    "        return q\n",
    "     \n",
    "    def sample(self, state):\n",
    "        if not isinstance(state[0], np.float32):\n",
    "            state = np.array(state[0])\n",
    "        q = self.q_network.predict(state)\n",
    "        return np.argmax(q)\n",
    "\n",
    "    def backward(self):\n",
    "        if len(self.buffer.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # step 9:\n",
    "        # Sample a batch of experiences from the buffer\n",
    "        batch = Experience(*self.buffer.sample_batch(self.batch_size))        \n",
    "        batch_mask = ~np.array(batch.done, dtype=bool)\n",
    "\n",
    "        next_state = np.stack(batch.next_state)\n",
    "        states = np.stack(batch.state)\n",
    "        actions = np.concatenate(batch.action)\n",
    "        rewards = np.concatenate(batch.reward)        \n",
    "\n",
    "        # step 10:\n",
    "        # Compute target values for each experience in the batch\n",
    "        target_values = tf.where(batch_mask, rewards, 0)\n",
    "        target_values = tf.where(~batch_mask, target_values + self.gamma * self.critic_target_network.predict(next_state), target_values)\n",
    "\n",
    "\n",
    "        # step 11:\n",
    "        # Compute predicted Q-values for the states and actions in the batch\n",
    "        predicted_q_values = self.critic_q_network.predict(states)\n",
    "        predicted_q_values = predicted_q_values[tf.range(len(actions)), actions]\n",
    "\n",
    "        # Update critic Q-network weights using the computed values (backward pass SGD on the MSE loss)\n",
    "        # self.critic_q_network.fit(np.stack(states, actions), target_values, epochs=1, verbose=0)\n",
    "        def loss_function_MSE(predicted_q_values, target_values):\n",
    "            _ret = target_values - predicted_q_values\n",
    "            return tf.reduce_mean(tf.square(_ret))\n",
    "        self.critic_q_network.update(self.optimizer, loss_function_MSE, predicted_q_values, target_values)\n",
    "\n",
    "        # step 12:\n",
    "        if self.steps % self.target_update_rate == self.target_update_rate - 1:\n",
    "            # step 13:\n",
    "            # update critic\n",
    "            def loss_function_J(states):\n",
    "                _ret = -self.critic_q_network.compute_q_values(states, self.actor_q_network.predict(states))\n",
    "                return tf.reduce_mean(_ret)\n",
    "            self.actor_q_network.update(self.optimizer, loss_function_J, states, target_values)\n",
    "\n",
    "            # step 14:\n",
    "            # stof update target networks\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        # step 16:\n",
    "        # Increment steps counter\n",
    "        self.steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n",
    "\n",
    "\n",
    "def faster_running_average(x, N, last_average):\n",
    "    if len(x) > N:\n",
    "        return last_average + (1./N)*(x[-1] - x[-N - 1])\n",
    "    else:\n",
    "        return sum(x)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 08:42:34.455640: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_episodes = 1000                 # Number of episodes # 100 - 1000\n",
    "discount_factor = 0.99            # Value of the discount factor\n",
    "n_ep_running_average = 50         # Running average of 50 episodes\n",
    "n_actions = 4                     # Number of available actions (nr of angles)\n",
    "dim_state = 2                     # State dimensionality\n",
    "replay_size = 5000                # 5000 - 30000\n",
    "batch_size = 64                   # 4 - 128\n",
    "learning_rate = 1e-4              # 1e-3 - 1e-4\n",
    "target_reward = 0                 # specified in lab\n",
    "max_env_steps = 1000              # to stop the episode\n",
    "\n",
    "# We will use these variables to compute the average episodic reward and\n",
    "# the average number of steps per episode\n",
    "episode_reward_list = []       # this list contains the total reward per episode\n",
    "episode_number_of_steps = []   # this list contains the number of steps per episode\n",
    "\n",
    "# agent initialization\n",
    "# agent = RandomAgent(n_actions) # random\n",
    "# step 1 & 2:\n",
    "agent = DDPGAgent(dim_state, n_actions, replay_size, batch_size=batch_size, gamma=discount_factor, learning_rate=learning_rate, n_episodes=N_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleControlledFixedEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "### Training process\n",
    "from tqdm import trange\n",
    "# trange is an alternative to range in python, from the tqdm library\n",
    "# It shows a nice progression bar that you can update with useful information\n",
    "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "actual_episodes = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate getting higher-quality training data, you may reduce the scale of the noise over the course of training. (We do not do this in our implementation, and keep noise scale fixed throughout.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At test time, to see how well the policy exploits what it has learned, we do not add noise to the actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DDPG implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning (set with the start_steps keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal DDPG exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[[0.19564064 0.06074065]]\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[[1]]\n",
      "passed state reshape\n",
      "[0.199595711118774, 0.05903001212455178]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_2/dense_6/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/root/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_6962/1418870914.py\", line 30, in <module>\n      action = agent.forward(state)\n    File \"/tmp/ipykernel_6962/4245071269.py\", line 53, in forward\n      q = self.actor_q_network.predict(state)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/core/dense.py\", line 255, in call\n      outputs = self.activation(outputs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/activations.py\", line 317, in relu\n      return backend.relu(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5369, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_2/dense_6/Relu'\nIn[0] is not a matrix\n\t [[{{node sequential_2/dense_6/Relu}}]] [Op:__inference_predict_function_632]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# step 8:\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Execute action in the environment and append\u001b[39;00m\n\u001b[1;32m     34\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m, in \u001b[0;36mDDPGAgent.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# step 7:\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# take noisy continuous action a_t at s_t   \u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_q_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# loop over the actions and add noise\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(q)):\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m# assign noise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_2/dense_6/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/root/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"/root/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/root/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_6962/1418870914.py\", line 30, in <module>\n      action = agent.forward(state)\n    File \"/tmp/ipykernel_6962/4245071269.py\", line 53, in forward\n      q = self.actor_q_network.predict(state)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/core/dense.py\", line 255, in call\n      outputs = self.activation(outputs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/activations.py\", line 317, in relu\n      return backend.relu(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5369, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_2/dense_6/Relu'\nIn[0] is not a matrix\n\t [[{{node sequential_2/dense_6/Relu}}]] [Op:__inference_predict_function_632]"
     ]
    }
   ],
   "source": [
    "avg_reward = 0.\n",
    "avg_steps = 0.\n",
    "# step 3:\n",
    "# episode loop\n",
    "for i in EPISODES:\n",
    "    # Reset environment data and initialize variables\n",
    "    done = False\n",
    "    # step 4:\n",
    "    state = env.reset()\n",
    "    state = np.array(state, dtype=np.float32)\n",
    " \n",
    "    total_episode_reward = 0.\n",
    "    # step 5:\n",
    "    t = 0\n",
    "    # step 6: \n",
    "    # environment loop\n",
    "    while not done:\n",
    "        # necessary for lunar lander. It doesn't implement a default\n",
    "        # max-timesteps and rover hovers forever\n",
    "\n",
    "        # step 7:\n",
    "        # noisy actor action\n",
    "        try:\n",
    "            state = state.reshape(1, dim_state)\n",
    "            print(state)\n",
    "        except:\n",
    "            print(\"passed state reshape\")\n",
    "            print(state)\n",
    "            pass\n",
    "        action = agent.forward(state)\n",
    "\n",
    "        # step 8:\n",
    "        # Execute action in the environment and append\n",
    "        next_state, reward, done = env.step(action)\n",
    "        # append to buffer\n",
    "        agent.buffer.append(Experience(state, action, np.array([reward]), next_state, done))\n",
    "        \n",
    "        # step 9-15: see function definition\n",
    "        agent.backward()\n",
    "        \n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # step 16: Update state for next iteration\n",
    "        state = next_state\n",
    "        t += 1\n",
    "\n",
    "    # Append episode reward and total number of steps\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "    episode_number_of_steps.append(t)\n",
    "\n",
    "    avg_reward = faster_running_average(episode_reward_list, n_ep_running_average, avg_reward)\n",
    "    avg_steps = faster_running_average(episode_number_of_steps, n_ep_running_average, avg_steps)\n",
    "\n",
    "    agent.episode += 1\n",
    "\n",
    "    # Updates the tqdm update bar with fresh information\n",
    "    # (episode number, total reward of the last episode, total number of Steps\n",
    "    # of the last episode, average reward, average number of steps)\n",
    "    EPISODES.set_description(\n",
    "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{:.1f}\".format(\n",
    "        i, total_episode_reward, t,\n",
    "        avg_reward,\n",
    "        avg_steps)\n",
    "        )\n",
    "\n",
    "    actual_episodes += 1\n",
    "    \n",
    "    # stop if we hit reward threshold\n",
    "    if avg_reward >= target_reward:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
