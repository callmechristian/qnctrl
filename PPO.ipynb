{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(\n",
    "        tf.random.categorical(logits, 1), axis=1\n",
    "    )\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 100\n",
    "epochs = 100\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 0.002\n",
    "value_function_learning_rate = 0.001\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (128, 64)\n",
    "\n",
    "# True if you want to render the environment\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=\"float32\")\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions])\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "value = tf.squeeze(mlp(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, _ = env.reset()\n",
    "episode_return, episode_length = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 1. Mean Return: -485.5967420785599. Mean Length: 100.0\n",
      " Epoch: 2. Mean Return: -484.9942384503566. Mean Length: 100.0\n",
      " Epoch: 3. Mean Return: -700.1540753203707. Mean Length: 100.0\n",
      " Epoch: 4. Mean Return: -707.2483316219166. Mean Length: 100.0\n",
      " Epoch: 5. Mean Return: -490.37084964981904. Mean Length: 100.0\n",
      " Epoch: 6. Mean Return: -614.8475030844817. Mean Length: 100.0\n",
      " Epoch: 7. Mean Return: -863.0535372249938. Mean Length: 100.0\n",
      " Epoch: 8. Mean Return: -623.9971191953692. Mean Length: 100.0\n",
      " Epoch: 9. Mean Return: -923.927241093567. Mean Length: 100.0\n",
      " Epoch: 10. Mean Return: -953.0771964718128. Mean Length: 100.0\n",
      " Epoch: 11. Mean Return: -485.66150899888373. Mean Length: 100.0\n",
      " Epoch: 12. Mean Return: -468.1268962055704. Mean Length: 100.0\n",
      " Epoch: 13. Mean Return: -474.4867017634522. Mean Length: 100.0\n",
      " Epoch: 14. Mean Return: -825.4703450296918. Mean Length: 100.0\n",
      " Epoch: 15. Mean Return: -373.6855559264717. Mean Length: 100.0\n",
      " Epoch: 16. Mean Return: -585.9956694238257. Mean Length: 100.0\n",
      " Epoch: 17. Mean Return: -892.6321743603812. Mean Length: 100.0\n",
      " Epoch: 18. Mean Return: -705.2651537866833. Mean Length: 100.0\n",
      " Epoch: 19. Mean Return: -373.71353477625473. Mean Length: 100.0\n",
      " Epoch: 20. Mean Return: -874.2483162208699. Mean Length: 100.0\n",
      " Epoch: 21. Mean Return: -579.2126217521796. Mean Length: 100.0\n",
      " Epoch: 22. Mean Return: -484.731889722969. Mean Length: 100.0\n",
      " Epoch: 23. Mean Return: -647.2687774992469. Mean Length: 100.0\n",
      " Epoch: 24. Mean Return: -587.7223500884744. Mean Length: 100.0\n",
      " Epoch: 25. Mean Return: -702.4819126020583. Mean Length: 100.0\n",
      " Epoch: 26. Mean Return: -487.48196137016566. Mean Length: 100.0\n",
      " Epoch: 27. Mean Return: -374.56153166421274. Mean Length: 100.0\n",
      " Epoch: 28. Mean Return: -477.9315200602059. Mean Length: 100.0\n",
      " Epoch: 29. Mean Return: -841.5516721052269. Mean Length: 100.0\n",
      " Epoch: 30. Mean Return: -496.5403061777898. Mean Length: 100.0\n",
      " Epoch: 31. Mean Return: -564.5472865659565. Mean Length: 100.0\n",
      " Epoch: 32. Mean Return: -650.5451907056631. Mean Length: 100.0\n",
      " Epoch: 33. Mean Return: -375.72129775938845. Mean Length: 100.0\n",
      " Epoch: 34. Mean Return: -301.76822734868983. Mean Length: 100.0\n",
      " Epoch: 35. Mean Return: -494.11194999374874. Mean Length: 100.0\n",
      " Epoch: 36. Mean Return: -652.279898099809. Mean Length: 100.0\n",
      " Epoch: 37. Mean Return: -845.1975397000604. Mean Length: 100.0\n",
      " Epoch: 38. Mean Return: -487.7271240045043. Mean Length: 100.0\n",
      " Epoch: 39. Mean Return: -581.2452792258541. Mean Length: 100.0\n",
      " Epoch: 40. Mean Return: -929.8040223431643. Mean Length: 100.0\n",
      " Epoch: 41. Mean Return: -488.44560262265486. Mean Length: 100.0\n",
      " Epoch: 42. Mean Return: -669.5380369367437. Mean Length: 100.0\n",
      " Epoch: 43. Mean Return: -918.3112975665905. Mean Length: 100.0\n",
      " Epoch: 44. Mean Return: -251.50784204792603. Mean Length: 100.0\n",
      " Epoch: 45. Mean Return: -763.0602298641463. Mean Length: 100.0\n",
      " Epoch: 46. Mean Return: -497.41179065285496. Mean Length: 100.0\n",
      " Epoch: 47. Mean Return: -535.4615118783448. Mean Length: 100.0\n",
      " Epoch: 48. Mean Return: -485.05410125777144. Mean Length: 100.0\n",
      " Epoch: 49. Mean Return: -693.1333463570566. Mean Length: 100.0\n",
      " Epoch: 50. Mean Return: -871.821117259079. Mean Length: 100.0\n",
      " Epoch: 51. Mean Return: -471.9151505191575. Mean Length: 100.0\n",
      " Epoch: 52. Mean Return: -616.1923504748996. Mean Length: 100.0\n",
      " Epoch: 53. Mean Return: -251.55600222030637. Mean Length: 100.0\n",
      " Epoch: 54. Mean Return: -365.1145051380665. Mean Length: 100.0\n",
      " Epoch: 55. Mean Return: -685.3913125643513. Mean Length: 100.0\n",
      " Epoch: 56. Mean Return: -482.81843631385976. Mean Length: 100.0\n",
      " Epoch: 57. Mean Return: -583.9571346712899. Mean Length: 100.0\n",
      " Epoch: 58. Mean Return: -780.5285062205833. Mean Length: 100.0\n",
      " Epoch: 59. Mean Return: -769.1012098005876. Mean Length: 100.0\n",
      " Epoch: 60. Mean Return: -875.2050453588411. Mean Length: 100.0\n",
      " Epoch: 61. Mean Return: -724.4197885147199. Mean Length: 100.0\n",
      " Epoch: 62. Mean Return: -531.4799363959579. Mean Length: 100.0\n",
      " Epoch: 63. Mean Return: -485.275461910394. Mean Length: 100.0\n",
      " Epoch: 64. Mean Return: -576.2800041134656. Mean Length: 100.0\n",
      " Epoch: 65. Mean Return: -436.1777529707575. Mean Length: 100.0\n",
      " Epoch: 66. Mean Return: -693.8321707520693. Mean Length: 100.0\n",
      " Epoch: 67. Mean Return: -595.1915290585166. Mean Length: 100.0\n",
      " Epoch: 68. Mean Return: -503.81220099914225. Mean Length: 100.0\n",
      " Epoch: 69. Mean Return: -502.89192033629394. Mean Length: 100.0\n",
      " Epoch: 70. Mean Return: -345.65314799822. Mean Length: 100.0\n",
      " Epoch: 71. Mean Return: -565.8518373090859. Mean Length: 100.0\n",
      " Epoch: 72. Mean Return: -592.5483417560513. Mean Length: 100.0\n",
      " Epoch: 73. Mean Return: -575.6423583155774. Mean Length: 100.0\n",
      " Epoch: 74. Mean Return: -816.5535860422359. Mean Length: 100.0\n",
      " Epoch: 75. Mean Return: -748.3569461113314. Mean Length: 100.0\n",
      " Epoch: 76. Mean Return: -374.1040015831062. Mean Length: 100.0\n",
      " Epoch: 77. Mean Return: -265.7763136311925. Mean Length: 100.0\n",
      " Epoch: 78. Mean Return: -907.5747611608691. Mean Length: 100.0\n",
      " Epoch: 79. Mean Return: -876.3139314209498. Mean Length: 100.0\n",
      " Epoch: 80. Mean Return: -581.0570780141745. Mean Length: 100.0\n",
      " Epoch: 81. Mean Return: -649.318817106027. Mean Length: 100.0\n",
      " Epoch: 82. Mean Return: -849.2010092120141. Mean Length: 100.0\n",
      " Epoch: 83. Mean Return: -374.46072514621704. Mean Length: 100.0\n",
      " Epoch: 84. Mean Return: -715.977683140051. Mean Length: 100.0\n",
      " Epoch: 85. Mean Return: -755.4316731587251. Mean Length: 100.0\n",
      " Epoch: 86. Mean Return: -588.1062515635192. Mean Length: 100.0\n",
      " Epoch: 87. Mean Return: -550.8280433453405. Mean Length: 100.0\n",
      " Epoch: 88. Mean Return: -577.1720223077879. Mean Length: 100.0\n",
      " Epoch: 89. Mean Return: -791.4744710035734. Mean Length: 100.0\n",
      " Epoch: 90. Mean Return: -759.8867533453697. Mean Length: 100.0\n",
      " Epoch: 91. Mean Return: -631.690718605. Mean Length: 100.0\n",
      " Epoch: 92. Mean Return: -904.066745284674. Mean Length: 100.0\n",
      " Epoch: 93. Mean Return: -257.8474757906524. Mean Length: 100.0\n",
      " Epoch: 94. Mean Return: -569.4600081451962. Mean Length: 100.0\n",
      " Epoch: 95. Mean Return: -694.4249229370703. Mean Length: 100.0\n",
      " Epoch: 96. Mean Return: -374.5117962401829. Mean Length: 100.0\n",
      " Epoch: 97. Mean Return: -748.6222294144275. Mean Length: 100.0\n",
      " Epoch: 98. Mean Return: -783.5939341639204. Mean Length: 100.0\n",
      " Epoch: 99. Mean Return: -374.65295438790764. Mean Length: 100.0\n",
      " Epoch: 100. Mean Return: -373.3318945639879. Mean Length: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        observation = observation.reshape(1, -1)\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _, _ = env.step([action[0].numpy()])\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, _ = env.reset()\n",
    "            episode_return, episode_length = 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    print(f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
